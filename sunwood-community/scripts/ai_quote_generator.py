#!/usr/bin/env python3
"""
AIè§£èª¬ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - éå»ãƒ­ã‚°ã‚’æ´»ç”¨ã—ãŸæ–‡è„ˆç†è§£

Usage:
    uv run ai_quote_generator.py <ãƒã‚¹ãƒˆURL>
    uv run ai_quote_generator.py <ãƒã‚¹ãƒˆURL> --preview
"""

import argparse
import json
import sys
from datetime import datetime, timezone
from pathlib import Path

import httpx

# è¨­å®š
COMMUNITY_ID = "2010195061309587967"  # Sunwood AI OSS Hub
TOKEN_FILE = Path(__file__).parent.parent.parent.parent / "x-tokens.json"
LOGS_DIR = Path(__file__).parent.parent / "logs"


def load_token() -> str:
    """ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã‚€"""
    if not TOKEN_FILE.exists():
        raise FileNotFoundError(f"Token file not found: {TOKEN_FILE}")

    with open(TOKEN_FILE) as f:
        data = json.load(f)
    return data.get("access_token", "")


def extract_tweet_id(url_or_id: str) -> str:
    """URLã¾ãŸã¯IDã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æŠ½å‡º"""
    if url_or_id.isdigit():
        return url_or_id

    from urllib.parse import urlparse

    parts = urlparse(url_or_id).path.split("/")
    for i, part in enumerate(parts):
        if part == "status" and i + 1 < len(parts):
            return parts[i + 1]

    raise ValueError(f"Invalid tweet URL or ID: {url_or_id}")


def get_tweet(tweet_id: str, token: str) -> dict:
    """ãƒ„ã‚¤ãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—"""
    url = f"https://api.x.com/2/tweets/{tweet_id}"
    headers = {"Authorization": f"Bearer {token}"}
    params = {"tweet.fields": "created_at,author_id,text", "expansions": "author_id", "user.fields": "name,username"}

    with httpx.Client() as client:
        resp = client.get(url, headers=headers, params=params)
        resp.raise_for_status()
        return resp.json()


def get_recent_logs(days: int = 7) -> list[dict]:
    """æœ€è¿‘ã®ãƒ­ã‚°ã‚’å–å¾—"""
    logs = []
    now = datetime.now(timezone.utc)

    for i in range(days):
        date = now - __import__("datetime").timedelta(days=i)
        date_dir = LOGS_DIR / date.strftime("%Y-%m-%d")
        if date_dir.exists():
            for log_file in sorted(date_dir.glob("*.json"), reverse=True):
                with open(log_file) as f:
                    logs.append(json.load(f))

    return logs[:20]  # æœ€å¤§20ä»¶


def analyze_context(tweet_text: str, author_name: str, recent_logs: list[dict]) -> dict:
    """éå»ãƒ­ã‚°ã‹ã‚‰æ–‡è„ˆã‚’åˆ†æ"""
    context = {
        "is_series": False,
        "series_count": 0,
        "related_topics": [],
        "previous_summaries": [],
    }

    # åŒã˜ä½œè€…ã®æŠ•ç¨¿ã‚’æ¢ã™
    author_posts = []
    for log in recent_logs:
        log_text = log.get("community_post", {}).get("text", "")
        if author_name.lower() in log_text.lower():
            author_posts.append(log)

    if author_posts:
        context["is_series"] = True
        context["series_count"] = len(author_posts)
        context["previous_summaries"] = [p.get("community_post", {}).get("text", "")[:100] for p in author_posts[:3]]

    # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    keywords = ["AGI", "AI", "LLM", "GPT", "Claude", "Gemini", "OpenAI", "Anthropic", "FUTODAMA", "OpenClaw", "ã‚¹ã‚­ãƒ«", "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    for kw in keywords:
        if kw.lower() in tweet_text.lower():
            context["related_topics"].append(kw)

    return context


def generate_smart_summary(tweet_text: str, author_name: str, context: dict) -> str:
    """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸã‚¹ãƒãƒ¼ãƒˆãªè§£èª¬ã‚’ç”Ÿæˆ"""

    # ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ãåˆ†é¡
    topics = context.get("related_topics", [])
    is_series = context.get("is_series", False)
    series_count = context.get("series_count", 0)

    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥ã®çµµæ–‡å­—ã¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    topic_emoji = {
        "AGI": "ğŸ”®",
        "AI": "ğŸ¤–",
        "LLM": "ğŸ§ ",
        "GPT": "ğŸ’¬",
        "Claude": "ğŸ”®",
        "Gemini": "ğŸ’",
        "OpenAI": "ğŸŒ",
        "Anthropic": "ğŸ”®",
        "FUTODAMA": "ğŸ ",
        "OpenClaw": "ğŸ¦",
        "ã‚¹ã‚­ãƒ«": "ğŸ­",
        "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": "ğŸ¤–",
    }

    # ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã‚’æ±ºå®š
    main_topic = topics[0] if topics else "AI"
    emoji = topic_emoji.get(main_topic, "ğŸ”")

    # ã‚·ãƒªãƒ¼ã‚ºã‚‚ã®ã®å ´åˆ
    if is_series:
        prefix = f"{emoji} {author_name}ã‚·ãƒªãƒ¼ã‚ºç¬¬{series_count + 1}å¼¾"
    else:
        prefix = f"{emoji} æ³¨ç›®ãƒã‚¹ãƒˆ"

    # å†…å®¹ã®è¦ç´„ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if len(tweet_text) > 200:
        summary = tweet_text[:200] + "..."
    else:
        summary = tweet_text

    return f"{prefix}\n\n{summary}"


def post_community_tweet(text: str, token: str) -> dict:
    """ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«æŠ•ç¨¿"""
    url = "https://api.x.com/2/tweets"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }
    payload = {"text": text, "community_id": COMMUNITY_ID}

    with httpx.Client() as client:
        resp = client.post(url, headers=headers, json=payload)
        resp.raise_for_status()
        return resp.json()


def save_log(original_tweet: dict, community_post: dict, quote_text: str):
    """æŠ•ç¨¿ãƒ­ã‚°ã‚’ä¿å­˜"""
    now = datetime.now(timezone.utc)
    date_dir = LOGS_DIR / now.strftime("%Y-%m-%d")
    date_dir.mkdir(parents=True, exist_ok=True)

    tweet_id = original_tweet.get("id", "unknown")
    log_file = date_dir / f"{now.strftime('%H-%M-%S')}_{tweet_id}.json"

    log_data = {
        "timestamp": now.isoformat(),
        "original_tweet": {
            "id": tweet_id,
            "text": original_tweet.get("text", ""),
            "url": f"https://x.com/i/status/{tweet_id}",
        },
        "community_post": {
            "id": community_post.get("data", {}).get("id", ""),
            "text": quote_text,
            "url": f"https://x.com/i/status/{community_post.get('data', {}).get('id', '')}",
        },
    }

    with open(log_file, "w") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ ãƒ­ã‚°ä¿å­˜: {log_file}")


def main():
    parser = argparse.ArgumentParser(description="AIè§£èª¬ç”Ÿæˆä»˜ãå¼•ç”¨ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ")
    parser.add_argument("tweet_url", help="å¼•ç”¨ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã®URLã¾ãŸã¯ID")
    parser.add_argument("--preview", action="store_true", help="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿")
    parser.add_argument("--days", type=int, default=7, help="éå»ãƒ­ã‚°å‚ç…§æ—¥æ•°")

    args = parser.parse_args()

    try:
        # ãƒˆãƒ¼ã‚¯ãƒ³èª­ã¿è¾¼ã¿
        token = load_token()
        if not token:
            print("âŒ ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)

        # ãƒ„ã‚¤ãƒ¼ãƒˆIDæŠ½å‡º
        tweet_id = extract_tweet_id(args.tweet_url)
        print(f"ğŸ“Œ ãƒ„ã‚¤ãƒ¼ãƒˆID: {tweet_id}")

        # ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—
        tweet_data = get_tweet(tweet_id, token)
        tweet = tweet_data.get("data", {})
        tweet_text = tweet.get("text", "")
        author = tweet_data.get("includes", {}).get("users", [{}])[0]
        author_name = author.get("name", "Unknown")
        print(f"ğŸ‘¤ ä½œè€…: {author_name}")
        print(f"ğŸ“ å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: {tweet_text[:100]}...")

        # éå»ãƒ­ã‚°å–å¾—ãƒ»åˆ†æ
        recent_logs = get_recent_logs(args.days)
        print(f"ğŸ“š éå»ãƒ­ã‚°: {len(recent_logs)}ä»¶")

        context = analyze_context(tweet_text, author_name, recent_logs)
        print(f"ğŸ” æ–‡è„ˆåˆ†æ: ã‚·ãƒªãƒ¼ã‚º={context['is_series']}, ãƒˆãƒ”ãƒƒã‚¯={context['related_topics']}")

        # ã‚¹ãƒãƒ¼ãƒˆè§£èª¬ç”Ÿæˆ
        summary = generate_smart_summary(tweet_text, author_name, context)

        # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        tweet_url = f"https://x.com/i/status/{tweet_id}"
        quote_text = f"{summary}\n\n{tweet_url}"

        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        print("\n" + "=" * 40)
        print("ğŸ“¤ æŠ•ç¨¿å†…å®¹:")
        print("=" * 40)
        print(quote_text)
        print("=" * 40 + "\n")

        if args.preview:
            print("ğŸ” ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰: æŠ•ç¨¿ã—ã¾ã›ã‚“")
            return

        # æŠ•ç¨¿å®Ÿè¡Œ
        result = post_community_tweet(quote_text, token)
        post_id = result.get("data", {}).get("id", "")
        print(f"âœ… æŠ•ç¨¿æˆåŠŸ: https://x.com/i/status/{post_id}")

        # ãƒ­ã‚°ä¿å­˜
        save_log(tweet, result, quote_text)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
